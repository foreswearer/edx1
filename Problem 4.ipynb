{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Capstone project notebook\n\n##\u00a0Problem 3", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### What Is the Relationship between Housing Characteristics and Complaints?\nThe goal of this exercise is to find the answer to the Question 3 of the problem statement: \n\n### Does the Complaint Type that you identified in response to Question 1 have an obvious relationship with any particular characteristic or characteristic of the Houses?\n\nIn this exercise, use the 311 dataset.\n\nYou also need to read back the PLUTO dataset from Cloud Object Store that you saved previously in the course. Use the PLUTO dataset for the borough that you already identified to focus on the last exercise.Ensure that you use only a limited number of fields from the dataset so that you are not consuming too much memory during your analysis.\n\nThe recommended fields are Address, BldgArea, BldgDepth, BuiltFAR, CommFAR, FacilFAR, Lot, LotArea, LotDepth, NumBldgs, NumFloors, OfficeArea, ResArea, ResidFAR, RetailArea, YearBuilt, YearAlter1, ZipCode, YCoord, and XCoord.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "### Read Bronx file\n\nTo prevent loading unnecesary data, we select the columns to load", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "body = client_cba83a820ee941cd921cc2bbfefd15eb.get_object(Bucket='edx1-donotdelete-pr-ffppmpbmudcobi',Key='bronxs.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\ncols_to_read = [  'Address',\n                  'BldgArea',\n                  'BldgDepth',\n                  'BuiltFAR',\n                  'CommFAR',\n                  'FacilFAR',\n                  'Lot', \n                  'LotArea',\n                  'LotDepth',\n                  'NumBldgs',\n                  'NumFloors',\n                  'OfficeArea',\n                  'ResArea',\n                  'ResidFAR',\n                  'RetailArea',\n                  'YearBuilt',\n                  'YearAlter1',\n                  'ZipCode',\n                  'YCoord',\n                  'XCoord']\ndf_bronx_info = pd.read_csv(body, usecols=cols_to_read)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_bronx_info.head()"
        }, 
        {
            "source": "### We create a subset pandas dataframe\n\nWith only the values to study, namely borough, address and location. After that we make and encoding setting BRONX as 1 and all others as 0\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_bronx_incidents = df_311[['complaint_type', 'incident_address', 'latitude', 'longitude', 'unique_key']].loc[df_311['borough'] == 'BRONX']\nprint('Number of Bronx incidents',df_bronx_incidents['unique_key'].count(),sep=' ')\ndf_bronx_incidents.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_bronx_incidents['complaint_type'] = (df_bronx_incidents['complaint_type'] == 'HEAT/HOT WATER').astype(int)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "import matplotlib.pyplot as plt\n%matplotlib inline\ndf_bronx_incidents.groupby('complaint_type').agg('complaint_type').count().plot.bar()"
        }, 
        {
            "source": "### We need to join incidents with building information\n\nA left inner join is what we need", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_bronx = pd.merge(df_bronx_incidents, df_bronx_info, how='left', left_on=['incident_address'], right_on=['Address'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print('Number of Bronx incidents and building information',df_bronx['unique_key'].count(),sep=' ')"
        }, 
        {
            "source": "### There are incident streets not available in the pluto file, so we just drop them\n\nThis is a problem with cardinality.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_bronx.dropna(inplace=True)"
        }, 
        {
            "source": "### Let's get rid of the addresses\n\nLet's see how many null values we have in the dataframe, and then eliminate them. Also we can select Lot as the index", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "df_bronx.drop(['Address', 'incident_address'], axis=1, inplace=True)"
        }, 
        {
            "source": "### Eliminate duplicates\n\nThere are a lot of duplicates. We are cleaning them and it results in *_1,211,609_* rows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_bronx.drop_duplicates(inplace=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print('Number of Bronx incidents with no duplicates',df_bronx['unique_key'].count(),sep=' ')"
        }, 
        {
            "source": "### Let's define dependant and independant variables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y = np.asarray(df_bronx['complaint_type'])\npredictors = df_bronx.columns.difference(['complaint_type'])\nX = df_bronx[predictors]\nX.set_index('unique_key', inplace=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "X.head(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y = np.asarray(df_bronx['complaint_type'])\npredictors = df_bronx.columns.difference(['complaint_type'])\nX = df_bronx[predictors]\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def xgb_model(train_data, train_label, test_data, test_label):\n    clf = xgb.XGBClassifier(max_depth=7,\n                           min_child_weight=1,\n                           learning_rate=0.1,\n                           n_estimators=500,\n                           silent=True,\n                           objective='binary:logistic',\n                           gamma=0,\n                           max_delta_step=0,\n                           subsample=1,\n                           colsample_bytree=1,\n                           colsample_bylevel=1,\n                           reg_alpha=0,\n                           reg_lambda=0,\n                           scale_pos_weight=1,\n                           seed=1,\n                           missing=None)\n    clf.fit(train_data, train_label, eval_metric='auc', verbose=True, eval_set=[(test_data, test_label)], early_stopping_rounds=100)\n    y_pre = clf.predict(test_data)\n    y_pro = clf.predict_proba(test_data)[:, 1]\n    return clf "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = xgb_model(X_train, y_train, X_test, y_test)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}