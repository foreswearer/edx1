{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Capstone project notebook\n\n##\u00a0Problem 3", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### What Is the Relationship between Housing Characteristics and Complaints?\nThe goal of this exercise is to find the answer to the Question 3 of the problem statement: \n\n### Does the Complaint Type that you identified in response to Question 1 have an obvious relationship with any particular characteristic or characteristic of the Houses?\n\nIn this exercise, use the 311 dataset.\n\nYou also need to read back the PLUTO dataset from Cloud Object Store that you saved previously in the course. Use the PLUTO dataset for the borough that you already identified to focus on the last exercise.Ensure that you use only a limited number of fields from the dataset so that you are not consuming too much memory during your analysis.\n\nThe recommended fields are Address, BldgArea, BldgDepth, BuiltFAR, CommFAR, FacilFAR, Lot, LotArea, LotDepth, NumBldgs, NumFloors, OfficeArea, ResArea, ResidFAR, RetailArea, YearBuilt, YearAlter1, ZipCode, YCoord, and XCoord.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "import types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_cba83a820ee941cd921cc2bbfefd15eb = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='56RsTx8E6jPTOm05rXcEWp-YR-lpps36XB-mae6-wNk9',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_cba83a820ee941cd921cc2bbfefd15eb.get_object(Bucket='edx1-donotdelete-pr-ffppmpbmudcobi',Key='nyc311.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_311 = pd.read_csv(body)\ndf_311.head()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Read Bronx file\n\nTo prevent loading unnecesary data, we select the columns to load", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "body = client_cba83a820ee941cd921cc2bbfefd15eb.get_object(Bucket='edx1-donotdelete-pr-ffppmpbmudcobi',Key='bronxs.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\ncols_to_read = [  'Address',\n                  'BldgArea',\n                  'BldgDepth',\n                  'BuiltFAR',\n                  'CommFAR',\n                  'FacilFAR',\n                  'Lot', \n                  'LotArea',\n                  'LotDepth',\n                  'NumBldgs',\n                  'NumFloors',\n                  'OfficeArea',\n                  'ResArea',\n                  'ResidFAR',\n                  'RetailArea',\n                  'YearBuilt',\n                  'YearAlter1',\n                  'ZipCode',\n                  'YCoord',\n                  'XCoord']\ndf_bronx_info = pd.read_csv(body, usecols=cols_to_read)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_bronx_info.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### We create a subset pandas dataframe\n\nWith only the values to study, namely borough, address and location. After that we make and encoding setting BRONX as 1 and all others as 0\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_bronx_incidents = df_311[['complaint_type', 'incident_address', 'latitude', 'longitude', 'unique_key']].loc[df_311['borough'] == 'BRONX']\nprint('Number of Bronx incidents',df_bronx_incidents['unique_key'].count(),sep=' ')\n#df_bronx_incidents.set_index('unique_key', inplace=True)\ndf_bronx_incidents.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_bronx_incidents['complaint_type'] = (df_bronx_incidents['complaint_type'] == 'HEAT/HOT WATER').astype(int)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import matplotlib.pyplot as plt\n%matplotlib inline\ndf_bronx_incidents.groupby('complaint_type').agg('complaint_type').count().plot.bar()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### We need to join incidents with building information\n\nA left inner join is what we need", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_bronx = pd.merge(df_bronx_incidents, df_bronx_info, how='left', left_on=['incident_address'], right_on=['Address'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print('Number of Bronx incidents and building information',df_bronx['unique_key'].count(),sep=' ')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### There are incident streets not available in the pluto file, so we just drop them\n\nThis is a problem with cardinality.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_bronx.dropna(inplace=True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Let's get rid of the addresses\n\nLet's see how many null values we have in the dataframe, and then eliminate them. Also we can select Lot as the index", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_bronx.drop(['Address', 'incident_address'], axis=1, inplace=True)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Eliminate duplicates\n\nThere are a lot of duplicates. We are cleaning them and it results in *_1,211,609_* rows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_bronx.drop_duplicates(inplace=True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print('Number of Bronx incidents with no duplicates',df_bronx['unique_key'].count(),sep=' ')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Let's define dependant and independant variables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\n\ny = np.asarray(df_bronx['complaint_type'])\npredictors = df_bronx.columns.difference(['complaint_type'])\nX = df_bronx[predictors]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "X.head(10)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from sklearn import preprocessing\nX_norm = preprocessing.StandardScaler().fit(X).transform(X)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from scipy.stats import pearsonr", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "###\u00a0Let's calculate correlations\n\nPearson correlation. As we want to know the importance as predictor, the absolute value is the indicator of correlation (ignoring if it is possitive or negative", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "correlations = []\nfor variable in df_bronx[df_bronx.columns.difference(['complaint_type'])]:\n    corr, p_value = pearsonr(y, X[variable])\n    correlations.append([variable,corr,p_value])\ndf_correlations = pd.DataFrame.from_records(correlations, columns=['predictor','Pearson correlation','p value'])\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_correlations.sort_values('Pearson correlation').head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_correlations.sort_values('Pearson correlation', ascending=False).head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Resulting in:\n\n1. NumFloors (+)\n2. BldgDepth (+)\n3. ResidFAR (+)\n3. FacilFAR (+)\n3. XCoord (-)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_bronx.plot.scatter(x='NumFloors', y='BldgDepth', c='complaint_type')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_bronx['NumFloors'].plot.bar()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}